"""
Entity extraction - extract cities, regions, counts from text.
Uses centralized patterns and priority-based matching.
"""
import re
from typing import List, Optional
from dataclasses import dataclass
import logging

from .patterns import PATTERNS
from .normalize import normalize_city, normalize_region, extract_region_from_alias, is_skip_word
from core.constants import CITIES, REGION_ALIASES, CHANNEL_REGIONS

REGION_ALIASES_LOWER = {k.lower() for k in REGION_ALIASES}
SUMMARY_COUNT_RE = re.compile(r'^\s*[Ğ-Ğ¯Ğ†Ğ‡Ğ„ÒĞ°-ÑÑ–Ñ—Ñ”Ò‘\s]+â€”\s*\d+Ñ…\s*$')
SUMMARY_HEADER_RE = re.compile(r'^\s*ĞŸĞ¾\s+Ğ‘Ğ¿Ğ›Ğ\b', re.IGNORECASE)
SPECIAL_ATTENTION_RE = re.compile(r'^ĞÑĞ¾Ğ±Ğ»Ğ¸Ğ²Ğ°\s+ÑƒĞ²Ğ°Ğ³Ğ°\s*:\s*(.*)$', re.IGNORECASE)
MAX_ATTENTION_ITEMS = 80

logger = logging.getLogger(__name__)


@dataclass
class ExtractedEntity:
    """Extracted location entity."""
    city: str
    region: str
    count: Optional[int] = None
    confidence: float = 1.0
    pattern_name: str = ""


def extract_entities(text: str, channel: str = None) -> List[ExtractedEntity]:
    """
    Extract all location entities from text.
    
    Uses priority-based pattern matching:
    1. Most specific patterns first (city + region in parens)
    2. Region header patterns (sets context)
    3. City-only patterns (use context or geocoding)
    """
    if not text:
        return []
    
    entities = []
    current_region = CHANNEL_REGIONS.get(channel) if channel else None
    
    special_attention = False
    attention_buffer: List[str] = []

    for line in text.strip().split('\n'):
        line = line.strip()
        if not line:
            if special_attention and attention_buffer:
                cities_text = ', '.join(attention_buffer)
                entities.extend(_build_entities_from_city_list(
                    cities_text,
                    None,
                    None,
                    0.7,
                    'special_attention'
                ))
                attention_buffer = []
            special_attention = False
            continue
        
        if PATTERNS.skip['alerts'].search(line) or PATTERNS.skip['shelter'].search(line):
            continue

        # Skip regional summary counts like "Ğ¡ÑƒĞ¼Ñ‰Ğ¸Ğ½Ğ° â€” 1Ñ…"
        if SUMMARY_COUNT_RE.match(line):
            continue
        if SUMMARY_HEADER_RE.match(line):
            continue

        # Parse "ĞÑĞ¾Ğ±Ğ»Ğ¸Ğ²Ğ° ÑƒĞ²Ğ°Ğ³Ğ°" blocks with city list
        attention_match = SPECIAL_ATTENTION_RE.match(line)
        if attention_match:
            special_attention = True
            tail = attention_match.group(1).strip()
            if tail:
                attention_buffer.append(tail)
            continue
        if special_attention:
            attention_buffer.append(line)
            if len(attention_buffer) >= MAX_ATTENTION_ITEMS:
                cities_text = ', '.join(attention_buffer)
                entities.extend(_build_entities_from_city_list(
                    cities_text,
                    None,
                    None,
                    0.7,
                    'special_attention'
                ))
                attention_buffer = []
                special_attention = False
            continue
        
        inline_header = _extract_inline_region_header(line)
        if inline_header:
            current_region, line = inline_header

        header_match = _extract_region_header(line)
        if header_match:
            current_region = header_match
            continue

        entity = _extract_city_region_parens(line)
        if entity:
            entities.append(entity)
            continue

        entity = _extract_city_region_alias_parens(line)
        if entity:
            entities.append(entity)
            continue
        
        region_cities = _extract_region_colon_cities(line, current_region)
        if region_cities:
            entities.extend(region_cities)
            continue
        
        context_entities = _extract_with_context(line, current_region)
        if context_entities:
            entities.extend(context_entities)
            continue
        
        arrow_entities = _extract_arrow_city(line, current_region)
        if arrow_entities:
            entities.extend(arrow_entities)
    
    if special_attention and attention_buffer:
        cities_text = ', '.join(attention_buffer)
        entities.extend(_build_entities_from_city_list(
            cities_text,
            None,
            None,
            0.7,
            'special_attention'
        ))

    return [e for e in entities if _is_valid_entity(e)]


def _extract_region_header(line: str) -> Optional[str]:
    clean = re.sub(r'^[âœˆï¸ğŸ›µğŸ›¸âš ï¸â—ï¸ğŸ”´ğŸ“¡\s]+', '', line).strip()
    match = re.match(r'^(\S+(?:\s+Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ)?):?\s*$', clean, re.IGNORECASE)
    if match:
        region_name = match.group(1).strip().rstrip(':')
        if region_name in REGION_ALIASES:
            return REGION_ALIASES[region_name]
        if 'Ğ¾Ğ±Ğ»Ğ°ÑÑ‚' in region_name.lower():
            return normalize_region(region_name)
    return None


def _extract_inline_region_header(line: str) -> Optional[tuple]:
    clean = re.sub(r'^[âœˆï¸ğŸ›µğŸ›¸âš ï¸â—ï¸ğŸ”´ğŸ“¡\s]+', '', line).strip()
    match = re.match(r'^(\S+(?:\s+Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ)?):\s*(.+)$', clean, re.IGNORECASE)
    if not match:
        return None

    region_name = match.group(1).strip().rstrip(':')
    remainder = match.group(2).strip()
    region = REGION_ALIASES.get(region_name)
    if not region and 'Ğ¾Ğ±Ğ»Ğ°ÑÑ‚' in region_name.lower():
        region = normalize_region(region_name)
    if not region:
        return None
    return region, remainder


def _extract_city_region_parens(line: str) -> Optional[ExtractedEntity]:
    match = PATTERNS.location['city_region_parens'].search(line)
    if not match:
        return None
    
    city_raw = match.group(1).strip()
    region_raw = match.group(2).strip()
    
    city = _clean_city_name(city_raw)
    if not city or is_skip_word(city):
        return None
    
    if city in REGION_ALIASES:
        return None
    
    city = normalize_city(city)
    region = normalize_region(region_raw)
    if not region:
        return None
    
    return ExtractedEntity(
        city=city,
        region=region,
        confidence=0.95,
        pattern_name='city_region_parens'
    )


def _extract_city_region_alias_parens(line: str) -> Optional[ExtractedEntity]:
    match = PATTERNS.location['city_region_alias_parens'].search(line)
    if not match:
        return None

    city_raw = match.group(1).strip()
    region_raw = match.group(2).strip()

    city = _clean_city_name(city_raw)
    if not city or is_skip_word(city):
        return None

    city = normalize_city(city)
    region = normalize_region(region_raw) or extract_region_from_alias(region_raw)
    if not region:
        return None

    return ExtractedEntity(
        city=city,
        region=region,
        confidence=0.9,
        pattern_name='city_region_alias_parens'
    )


def _extract_region_colon_cities(line: str, default_region: str = None) -> List[ExtractedEntity]:
    match = PATTERNS.location['region_colon_cities'].search(line)
    if not match:
        return []
    
    region_name = match.group(1).strip()
    cities_part = match.group(2).strip()
    
    region = REGION_ALIASES.get(region_name)
    if not region and 'Ğ¾Ğ±Ğ»Ğ°ÑÑ‚' in region_name.lower():
        region = normalize_region(region_name)
    if not region:
        region = default_region
    if not region:
        return []
    
    entities = []
    for entry in re.split(r',\s*', cities_part):
        city = _extract_city_from_entry(entry)
        if city and not is_skip_word(city):
            city = normalize_city(city)
            entities.append(ExtractedEntity(
                city=city,
                region=region,
                confidence=0.9,
                pattern_name='region_colon_cities'
            ))
    
    return entities


def _extract_with_context(line: str, current_region: str) -> List[ExtractedEntity]:
    region = current_region or extract_region_from_alias(line)
    if not region:
        return []

    entities: List[ExtractedEntity] = []
    
    match = PATTERNS.location['count_threat_na_city'].search(line)
    if match:
        count = int(match.group(1))
        cities_text = match.group(2)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                count,
                0.85,
                'count_threat_na_city'
            ))
            return entities

    match = PATTERNS.location['count_na_city'].search(line)
    if match:
        count = int(match.group(1))
        cities_text = match.group(2)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                count,
                0.8,
                'count_na_city'
            ))
            return entities

    match = PATTERNS.location['count_city'].search(line)
    if match:
        count = int(match.group(1))
        cities_text = match.group(2)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                count,
                0.75,
                'count_city'
            ))
            return entities

    match = PATTERNS.location['kursom_na_city'].search(line)
    if match:
        cities_text = match.group(1)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                None,
                0.75,
                'kursom_na_city'
            ))
            return entities

    match = PATTERNS.location['moves_to_city'].search(line)
    if match:
        cities_text = match.group(1)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                None,
                0.75,
                'moves_to_city'
            ))
            return entities
    
    match = PATTERNS.location['bpla_kursom_na'].search(line)
    if match:
        cities_text = match.group(1)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                None,
                0.85,
                'bpla_kursom_na'
            ))
            return entities
    
    match = PATTERNS.location['n_v_rayoni'].search(line)
    if match:
        count = int(match.group(1))
        cities_text = match.group(2)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                count,
                0.8,
                'n_v_rayoni'
            ))
            return entities

    match = PATTERNS.location['threat_bilya_city'].search(line)
    if match:
        cities_text = match.group(1)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                None,
                0.8,
                'threat_bilya_city'
            ))
            return entities

    match = PATTERNS.location['v_bik_city'].search(line)
    if match:
        cities_text = match.group(1)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                None,
                0.75,
                'v_bik_city'
            ))
            return entities

    match = PATTERNS.location['v_rayoni_city'].search(line)
    if match:
        cities_text = match.group(1)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                None,
                0.75,
                'v_rayoni_city'
            ))
            return entities

    match = PATTERNS.location['threat_nad_city'].search(line)
    if match:
        cities_text = match.group(1)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                None,
                0.8,
                'threat_nad_city'
            ))
            return entities

    match = PATTERNS.location['po_shahedu_na'].search(line)
    if match:
        cities_text = match.group(1)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                1,
                0.75,
                'po_shahedu_na'
            ))
            return entities

    match = PATTERNS.location['city_to_you'].search(line)
    if match:
        cities_text = match.group(1)
        if cities_text:
            entities.extend(_build_entities_from_city_list(
                cities_text,
                region,
                None,
                0.75,
                'city_to_you'
            ))
            return entities

    return entities


def _extract_arrow_city(line: str, current_region: str) -> List[ExtractedEntity]:
    region = current_region or extract_region_from_alias(line)
    if not region:
        return []
    
    match = PATTERNS.location['arrow_city'].match(line)
    if not match:
        arrow_index = max(line.rfind('â†’'), line.rfind('â¡ï¸'))
        if arrow_index >= 0:
            content = line[arrow_index + 1:].strip()
        else:
            return []
    else:
        content = match.group(1).strip()
    
    if content in REGION_ALIASES:
        return []
    
    cities = _split_cities(content)
    entities: List[ExtractedEntity] = []
    for city in cities:
        city = _clean_city_name(city)
        if not city or is_skip_word(city):
            continue
        entities.append(ExtractedEntity(
            city=normalize_city(city),
            region=region,
            confidence=0.8,
            pattern_name='arrow_city'
        ))

    return entities


def _extract_city_from_entry(entry: str) -> Optional[str]:
    entry = entry.strip()
    
    match = re.match(r'^\d+\s+(?:Ğ½Ğ°|Ğ² Ñ€Ğ°Ğ¹Ğ¾Ğ½Ñ–|Ğ±Ñ–Ğ»Ñ|Ğ¿Ğ¾Ğ²Ğ·)\s+(.+?)$', entry, re.IGNORECASE)
    if match:
        return match.group(1).strip().rstrip('.,;')
    
    match = re.match(r'^\d+\s+([Ğ-Ğ¯Ğ†Ğ‡Ğ„ÒĞ°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+)$', entry, re.IGNORECASE)
    if match:
        return match.group(1).strip().rstrip('.,;')
    
    match = re.match(r'^\d+\s*Ñ…?\s*ÑˆĞ°Ñ…ĞµĞ´[Ñ–Ğ¸Ñ–Ğ²]*\s+Ğ½Ğ°\s+(.+?)$', entry, re.IGNORECASE)
    if match:
        return match.group(1).strip().rstrip('.,;')
    
    match = re.match(r'^(?:Ğ‘Ğ¿Ğ›Ğ|Ğ‘ĞŸĞ›Ğ)\s+ĞºÑƒÑ€ÑĞ¾Ğ¼\s+Ğ½Ğ°\s+(.+?)$', entry, re.IGNORECASE)
    if match:
        return match.group(1).strip().rstrip('.,;')

    match = re.match(r'^\d+\s+(?:Ğ±Ñ–Ğ»Ñ|Ğ¿Ğ¾Ğ±Ğ»Ğ¸Ğ·Ñƒ)\s+(.+?)$', entry, re.IGNORECASE)
    if match:
        return match.group(1).strip().rstrip('.,;')

    match = re.match(r'^(?:ĞºÑ€ÑƒĞ¶Ğ»ÑÑ”|ĞºÑ€ÑƒÑ‚Ğ¸Ñ‚ÑŒÑÑ)\s+Ğ±Ñ–Ğ»Ñ\s+(.+?)$', entry, re.IGNORECASE)
    if match:
        return match.group(1).strip().rstrip('.,;')

    match = re.match(r'^(?:ÑˆĞ°Ñ…ĞµĞ´|Ğ‘Ğ¿Ğ›Ğ|Ğ‘ĞŸĞ›Ğ)\s+Ğ½Ğ°Ğ´\s+(.+?)$', entry, re.IGNORECASE)
    if match:
        return match.group(1).strip().rstrip('.,;')
    
    return None


def _clean_city_name(city: str) -> str:
    if not city:
        return ""
    
    city = city.strip()
    city = re.sub(r'^[ğŸ’¥ğŸ›¸ğŸ›µâš ï¸â—ï¸ğŸ”´ğŸš€âœˆï¸ğŸ‘ï¸â€¢â–ªï¸\*\s]+', '', city)
    city = re.sub(r'\([^)]*\)?', '', city).strip()  # Remove incomplete parens too
    city = re.sub(r'[ğŸ’¥ğŸ›¸ğŸ›µâš ï¸â—ï¸ğŸ”´ğŸš€âœˆï¸ğŸ‘ï¸]+', '', city)
    city = re.sub(r'^\d+\s*Ñ…?\s*', '', city)
    city = re.sub(r'^(?:Ğ‘ĞŸĞ›Ğ|Ğ‘Ğ¿Ğ›Ğ|Ğ‘ĞŸĞ›A|ÑˆĞ°Ñ…ĞµĞ´[Ñ–Ğ¸Ñ–Ğ²]*)\s*', '', city, flags=re.IGNORECASE)
    city = re.sub(r'^(?:Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ–Ğ¹|ĞºÑ€ÑƒÑ‚Ğ¸Ñ‚ÑŒÑÑ|ĞºÑ€ÑƒĞ¶Ğ»ÑÑ”|ĞºÑ€ÑƒĞ¶Ğ»ÑÑÑ‚ÑŒ|Ğ¼Ğ°Ğ½ĞµĞ²Ñ€ÑƒÑ”|Ğ¼Ğ°Ğ½ĞµĞ²Ñ€ÑƒÑÑ‚ÑŒ)\s+', '', city, flags=re.IGNORECASE)
    city = re.sub(r'^(?:Ğ¼Ñ–Ğ¶|Ğ¿Ğ¾Ğ¼Ñ–Ğ¶)\s+', '', city, flags=re.IGNORECASE)
    # Clean movement phrases
    city = re.sub(r'^(?:Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ²Ğ¶ÑƒÑ”\s+Ñ€ÑƒÑ…\s+Ğ½Ğ°|Ñƒ\s+Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ĞºÑƒ|Ğ²\s+Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ĞºÑƒ|Ğ½Ğ°|Ñ€ÑƒÑ…\s+Ğ½Ğ°)\s+', '', city, flags=re.IGNORECASE)
    city = re.sub(r'^(?:Ğ»ĞµÑ‚ÑÑ‚\s+Ğ²\s+ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ|Ğ»ĞµÑ‚Ğ¸Ñ‚ÑŒ\s+Ğ½Ğ°|Ğ¿Ğ¾ĞºĞ°|Ğ¿Ğ¾ĞºĞ¸)\s+', '', city, flags=re.IGNORECASE)
    city = re.sub(r'\s+Ğ·\s+\S+Ñ‰Ğ¸Ğ½[Ğ¸Ñ–Ñƒ]?\s*$', '', city, flags=re.IGNORECASE)
    city = re.sub(r'\s+Ğ·\s+\S+Ñ‡Ñ‡Ğ¸Ğ½[Ğ¸Ñ–Ñƒ]?\s*$', '', city, flags=re.IGNORECASE)
    city = re.sub(r'\s+Ğ·\s+Ñ‡Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾\s+Ğ¼Ğ¾Ñ€Ñ\s*$', '', city, flags=re.IGNORECASE)
    city = re.sub(r'\s+[Ğ²Ñƒ]\s+Ğ±Ñ–Ğº\s+.+$', '', city, flags=re.IGNORECASE)
    city = re.sub(r'\s+ĞºÑƒÑ€ÑĞ¾Ğ¼\s+Ğ½Ğ°\s+.+$', '', city, flags=re.IGNORECASE)
    if ' Ñ‚Ğ° ' in city:
        city = city.split(' Ñ‚Ğ° ')[0].strip()
    city = city.strip().rstrip('.,;!?')

    city_lower = city.lower()
    # Skip garbage
    if len(city) < 3:
        return ""
    # Skip common non-city words
    if city_lower in ('Ğ½Ğ°', 'Ğ½Ğ°Ğ´', 'Ğ¿Ñ–Ğ´', 'Ğ´Ğ¾', 'Ğ²Ñ–Ğ´', 'Ñ‡ĞµÑ€ĞµĞ·', 'Ğ±Ñ–Ğ»Ñ', 'ĞºĞ¾Ğ»Ğ¾', 'Ñ€ÑƒÑ…', 'ĞºÑƒÑ€Ñ', 'ĞºÑƒÑ€ÑĞ¾Ğ¼'):
        return ""
    if 'Ğ½ĞµĞ²Ğ¸Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¾Ğ³Ğ¾' in city_lower and 'Ñ‚Ğ¸Ğ¿' in city_lower:
        return ""
    if 'Ğ±Ğ¿Ğ»Ğ°' in city_lower or 'ÑˆĞ°Ñ…ĞµĞ´' in city_lower:
        return ""
    if city in REGION_ALIASES or city_lower in {k.lower() for k in REGION_ALIASES}:
        return ""
    if city_lower.endswith('Ñ‰Ğ¸Ğ½Ğ°') or city_lower.endswith('Ñ‡Ñ‡Ğ¸Ğ½Ğ°') or city_lower.endswith('Ñ‰Ğ¸Ğ½Ğ¸'):
        return ""
    if 'Ğ¼ĞµĞ¶Ñ–' in city_lower or 'Ğ¼ĞµĞ¶Ğ°' in city_lower:
        return ""
    if city_lower.startswith('Ğ· '):
        return ""
    # Skip districts (Ñ€Ğ°Ğ¹Ğ¾Ğ½Ñƒ, Ñ€Ğ°Ğ¹Ğ¾Ğ½)
    if 'Ñ€Ğ°Ğ¹Ğ¾Ğ½' in city_lower:
        return ""
    # Skip phrases not cities
    if 'Ñ†ĞµĞ½Ñ‚Ñ€ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ñ–' in city_lower or 'Ğ¼Ğ°Ğ½ĞµĞ²Ñ€' in city_lower:
        return ""
    # Skip Russian phrases
    if 'ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ' in city_lower or 'Ğ»ĞµÑ‚ÑÑ‚' in city_lower or 'Ğ¿Ğ¾ĞºĞ°' in city_lower:
        return ""
    # Skip Russian city forms (should be Ukrainian)
    if city_lower.endswith('ÑĞºĞ°') or city_lower.endswith('ÑĞºĞ¾Ğ³Ğ¾'):
        return ""
    # Skip incomplete words like "ĞÑ€Ğ¸Ğ¹" (truncated "Ğ¡Ñ‚Ğ°Ñ€Ğ¸Ğ¹")
    if city_lower in ('Ğ°Ñ€Ğ¸Ğ¹', 'Ğ°Ñ€Ñ–Ğ¹', 'Ğ¾Ğ²Ğ¸Ğ¹', 'Ğ¸Ğ¹', 'Ñ–Ğ¹'):
        return ""
    if city_lower.startswith('Ğ°Ñ€Ğ¸Ğ¹ ') or city_lower.startswith('Ğ°Ñ€Ñ–Ğ¹ '):
        return ""

    return city


def _split_cities(content: str) -> List[str]:
    parts = re.split(r'\s*(?:,|\s+Ñ‚Ğ°\s+|/)\s*', content)
    filtered = []
    for part in parts:
        if not part:
            continue
        low = part.lower()
        if low in ['Ñ€-Ğ½', 'Ñ€-Ğ½Ñƒ', 'Ñ€-Ğ½Ğ°', 'Ñ€Ğ°Ğ¹Ğ¾Ğ½', 'Ğ¾ĞºĞ¾Ğ»Ğ¸Ñ†Ñ–']:
            continue
        if part in REGION_ALIASES or low in REGION_ALIASES_LOWER:
            continue
        filtered.append(part)
    return filtered


def _build_entities_from_city_list(
    cities_text: str,
    region: str,
    count: Optional[int],
    confidence: float,
    pattern: str
) -> List[ExtractedEntity]:
    entities: List[ExtractedEntity] = []
    for city in _split_cities(cities_text):
        if any(dash in city for dash in ['-', 'â€“', 'â€”']):
            parts = re.split(r'\s*[-â€“â€”]\s*', city, maxsplit=1)
            if len(parts) == 2:
                left, right = [c.strip() for c in parts]
                left_norm = normalize_city(left, use_ai=False) or left
                right_norm = normalize_city(right, use_ai=False) or right
                if left_norm in CITIES and right_norm in CITIES:
                    for part in (left_norm, right_norm):
                        entities.extend(_build_entities_from_city_list(
                            part,
                            region,
                            count,
                            confidence,
                            pattern
                        ))
                    continue
        direction_match = PATTERNS.location['v_bik_city'].search(city)
        if direction_match:
            city = direction_match.group(1)
        city = _clean_city_name(city)
        if not city or is_skip_word(city):
            continue
        normalized_city = normalize_city(city)
        resolved_region = region or get_region_for_city(normalized_city, region)
        entities.append(ExtractedEntity(
            city=normalized_city,
            region=resolved_region,
            count=count,
            confidence=confidence,
            pattern_name=pattern
        ))
    return entities


def _is_valid_entity(entity: ExtractedEntity) -> bool:
    if not entity.city or not entity.region:
        return False
    if len(entity.city) < 2:
        return False
    if PATTERNS.direction_words.match(entity.city):
        return False
    if is_skip_word(entity.city):
        return False
    if entity.city in REGION_ALIASES:
        return False
    return True


def get_region_for_city(city: str, hint_region: str = None) -> Optional[str]:
    if city in CITIES:
        return CITIES[city]
    city_cap = city[0].upper() + city[1:] if city else city
    if city_cap in CITIES:
        return CITIES[city_cap]
    return hint_region
